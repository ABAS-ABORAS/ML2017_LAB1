{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expeiment 2: Linear Classification and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Output Of Training For 100 epoch ******************* \n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 0: \n",
      "0.89466524155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 0: \n",
      "0.484290639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 1: \n",
      "0.89323164155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 1: \n",
      "0.484373839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 2: \n",
      "0.89179804155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 2: \n",
      "0.484457039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 3: \n",
      "0.89036444155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 3: \n",
      "0.484540239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 4: \n",
      "0.88893084155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 4: \n",
      "0.484623439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 5: \n",
      "0.88749724155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 5: \n",
      "0.484706639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 6: \n",
      "0.88606364155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 6: \n",
      "0.484789839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 7: \n",
      "0.88463004155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 7: \n",
      "0.484873039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 8: \n",
      "0.88319644155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 8: \n",
      "0.484956239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 9: \n",
      "0.88176284155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 9: \n",
      "0.485039439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 10: \n",
      "0.88032924155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 10: \n",
      "0.485122639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 11: \n",
      "0.87889564155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 11: \n",
      "0.485205839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 12: \n",
      "0.87746204155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 12: \n",
      "0.485289039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 13: \n",
      "0.87602844155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 13: \n",
      "0.485372239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 14: \n",
      "0.87459484155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 14: \n",
      "0.485455439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 15: \n",
      "0.87316124155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 15: \n",
      "0.485538639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 16: \n",
      "0.87172764155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 16: \n",
      "0.485621839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 17: \n",
      "0.87029404155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 17: \n",
      "0.485705039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 18: \n",
      "0.86886044155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 18: \n",
      "0.485788239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 19: \n",
      "0.86742684155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 19: \n",
      "0.485871439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 20: \n",
      "0.86599324155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 20: \n",
      "0.485954639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 21: \n",
      "0.86455964155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 21: \n",
      "0.486037839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 22: \n",
      "0.86312604155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 22: \n",
      "0.486121039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 23: \n",
      "0.86169244155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 23: \n",
      "0.486204239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 24: \n",
      "0.86025884155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 24: \n",
      "0.486287439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 25: \n",
      "0.85882524155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 25: \n",
      "0.486370639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 26: \n",
      "0.85739164155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 26: \n",
      "0.486453839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 27: \n",
      "0.85595804155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 27: \n",
      "0.486537039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 28: \n",
      "0.85452444155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 28: \n",
      "0.486620239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 29: \n",
      "0.85309084155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 29: \n",
      "0.486703439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 30: \n",
      "0.85165724155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 30: \n",
      "0.486786639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 31: \n",
      "0.85022364155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 31: \n",
      "0.486869839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 32: \n",
      "0.84879004155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 32: \n",
      "0.486953039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 33: \n",
      "0.84735644155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 33: \n",
      "0.487036239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 34: \n",
      "0.84592284155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 34: \n",
      "0.487119439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 35: \n",
      "0.84448924155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 35: \n",
      "0.487202639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 36: \n",
      "0.84305564155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 36: \n",
      "0.487285839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 37: \n",
      "0.84162204155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 37: \n",
      "0.487369039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 38: \n",
      "0.84018844155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 38: \n",
      "0.487452239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 39: \n",
      "0.83875484155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 39: \n",
      "0.487535439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 40: \n",
      "0.83732124155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 40: \n",
      "0.487618639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 41: \n",
      "0.83588764155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 41: \n",
      "0.487701839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 42: \n",
      "0.83445404155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 42: \n",
      "0.487785039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 43: \n",
      "0.83302044155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 43: \n",
      "0.487868239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 44: \n",
      "0.83158684155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 44: \n",
      "0.487951439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 45: \n",
      "0.83015324155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 45: \n",
      "0.488034639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 46: \n",
      "0.82871964155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 46: \n",
      "0.488117839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 47: \n",
      "0.82728604155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 47: \n",
      "0.488201039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 48: \n",
      "0.82585244155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 48: \n",
      "0.488284239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 49: \n",
      "0.82441884155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 49: \n",
      "0.488367439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 50: \n",
      "0.82298524155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 50: \n",
      "0.488450639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 51: \n",
      "0.82155164155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 51: \n",
      "0.488533839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 52: \n",
      "0.82011804155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 52: \n",
      "0.488617039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 53: \n",
      "0.81868444155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 53: \n",
      "0.488700239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 54: \n",
      "0.81725084155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 54: \n",
      "0.488783439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 55: \n",
      "0.81581724155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 55: \n",
      "0.488866639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 56: \n",
      "0.81438364155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 56: \n",
      "0.488949839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 57: \n",
      "0.81295004155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 57: \n",
      "0.489033039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 58: \n",
      "0.81151644155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 58: \n",
      "0.489116239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 59: \n",
      "0.81008284155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 59: \n",
      "0.489199439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 60: \n",
      "0.80864924155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 60: \n",
      "0.489282639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 61: \n",
      "0.80721564155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 61: \n",
      "0.489365839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 62: \n",
      "0.80578204155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 62: \n",
      "0.489449039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 63: \n",
      "0.80434844155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 63: \n",
      "0.489532239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 64: \n",
      "0.80291484155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 64: \n",
      "0.489615439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 65: \n",
      "0.80148124155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 65: \n",
      "0.489698639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 66: \n",
      "0.80004764155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 66: \n",
      "0.489781839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 67: \n",
      "0.79861404155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 67: \n",
      "0.489865039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 68: \n",
      "0.79718044155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 68: \n",
      "0.489948239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 69: \n",
      "0.79574684155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 69: \n",
      "0.490031439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 70: \n",
      "0.79431324155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 70: \n",
      "0.490114639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 71: \n",
      "0.79287964155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 71: \n",
      "0.490197839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 72: \n",
      "0.79144604155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 72: \n",
      "0.490281039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 73: \n",
      "0.79001244155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 73: \n",
      "0.490364239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 74: \n",
      "0.78857884155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 74: \n",
      "0.490447439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 75: \n",
      "0.78714524155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 75: \n",
      "0.490530639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 76: \n",
      "0.78571164155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 76: \n",
      "0.490613839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 77: \n",
      "0.78427804155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 77: \n",
      "0.490697039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 78: \n",
      "0.78284444155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 78: \n",
      "0.490780239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 79: \n",
      "0.78141084155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 79: \n",
      "0.490863439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 80: \n",
      "0.77997724155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 80: \n",
      "0.490946639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 81: \n",
      "0.77854364155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 81: \n",
      "0.491029839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 82: \n",
      "0.77711004155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 82: \n",
      "0.491113039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 83: \n",
      "0.77567644155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 83: \n",
      "0.491196239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 84: \n",
      "0.77424284155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 84: \n",
      "0.491279439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 85: \n",
      "0.77280924155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 85: \n",
      "0.491362639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 86: \n",
      "0.77137564155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 86: \n",
      "0.491445839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 87: \n",
      "0.76994204155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 87: \n",
      "0.491529039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 88: \n",
      "0.76850844155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 88: \n",
      "0.491612239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 89: \n",
      "0.76707484155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 89: \n",
      "0.491695439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 90: \n",
      "0.76564124155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 90: \n",
      "0.491778639604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 91: \n",
      "0.76420764155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 91: \n",
      "0.491861839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 92: \n",
      "0.76277404155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 92: \n",
      "0.491945039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 93: \n",
      "0.76134044155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 93: \n",
      "0.492028239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 94: \n",
      "0.75990684155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 94: \n",
      "0.492111439604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 95: \n",
      "0.75847324155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 95: \n",
      "0.492194639604\n",
      "\n",
      " TRAIN_LOSS \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96: \n",
      "0.75703964155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 96: \n",
      "0.492277839604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 97: \n",
      "0.75560604155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 97: \n",
      "0.492361039604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 98: \n",
      "0.75417244155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 98: \n",
      "0.492444239604\n",
      "\n",
      " TRAIN_LOSS \n",
      "epoch 99: \n",
      "0.75273884155\n",
      "\n",
      " VALIADATION_LOSS \n",
      "epoch 99: \n",
      "0.492527439604\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******************************************************************* \n",
      "This Result Experiment 2 Linear Classification and Gradient Descent \n",
      "******************************************************************** \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x842d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def compute_loss(x,y,k):\n",
    "    n = x.shape[1]\n",
    "    total = 0\n",
    "    for z in range(x.shape[0]):\n",
    "        if np.sum((1 - y[z] * (x[z] * w + b[z]))) > 0:\n",
    "            total += np.sum((1 - y[z] * (x[z] * w + b[z])))\n",
    "    loss = np.sum(np.square(w)) / (2*n) + C * total\n",
    "    print('epoch '+ str(k) + ': ')\n",
    "    print(loss)\n",
    "    return loss\n",
    "\n",
    "# This  step am get Dataset from my computer \n",
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(\"C:/ABAS/DATA/australian_scale.txt\")\n",
    "x, y = data[0], data[1]\n",
    "# THIS Step : ***************   Devided The Dataset into Traning set and Validation Set   **********\n",
    "x_train,x_validation,y_train,y_validation = train_test_split(x,y,test_size=0.35,random_state=44)\n",
    "x_train,x_validation,y_train,y_validation = x_train.todense(),x_validation.todense(),y_train.reshape(len(y_train),-1),y_validation.reshape(len(y_validation),-1)\n",
    "\n",
    "#initialize b,w\n",
    "b = np.zeros((x_train.shape[0],1))\n",
    "w = np.empty((x_train.shape[1],1))\n",
    "\n",
    "# training\n",
    "iteration = 100 # number of epoch\n",
    "learning_rate = 0.8\n",
    "C = 0.002\n",
    "train_loss=[] #  Training loss\n",
    "validation_loss=[] #     Validation loss\n",
    "\n",
    "#*************************************************************************************\n",
    "print(\"************ Output Of Training For 100 epoch ******************* \")\n",
    "for i in range(iteration):\n",
    "  \n",
    "    for j in range(x_train.shape[0]):\n",
    "        if np.sum((1 - y_train[j] * (x_train[j] * w + b[j] ))) > 0:\n",
    "            w_gradient = w + x_train[j].T * C * (-1 * y_train[j])\n",
    "            b_gradient = -1 * C * y_train[j]\n",
    "        else:\n",
    "            w_gradient = w\n",
    "            b_gradient = 0\n",
    "        w = w - learning_rate * w_gradient\n",
    "        b[j] = b[j] - learning_rate * b_gradient\n",
    "        \n",
    "    print('\\n TRAIN_LOSS ')\n",
    "    train_loss.append( compute_loss(x_train,y_train,i) )\n",
    "    print('\\n VALIADATION_LOSS ')\n",
    "  \n",
    "    validation_loss.append( compute_loss(x_validation,y_validation,i) )\n",
    "    \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "#************************************************************************************************    \n",
    "\n",
    "# # This Drawing Graph of L_train  as well as L_validation with the Number of iterations\n",
    "import matplotlib.pyplot as PLO  # Import this fuction to draw graph\n",
    "t = np.arange(0, iteration, 1)\n",
    "PLO.plot(t, validation_loss, color=\"blue\", linewidth=2.5, linestyle=\"-\", label=\"Validation\")\n",
    "PLO.plot(t, train_loss, color=\"green\",  linewidth=2.5, linestyle=\"-\", label=\"Train\")\n",
    "PLO.legend(loc='upper right')\n",
    "PLO.plot(t, train_loss, 'g--',t, validation_loss, 'b--')\n",
    "PLO.xlabel('Iteration')\n",
    "PLO.ylabel('Cost')\n",
    "print(\"******************************************************************* \")\n",
    "print(\"This Result Experiment 2 Linear Classification and Gradient Descent \")\n",
    "print(\"******************************************************************** \")\n",
    "PLO.title(\"Graph Of Training Epoch Show Classification \")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
